{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData\n",
    "!pip install -qU transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from jinja2 import Template\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cwd = Path.cwd()\n",
    "cwd = r'C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data' #TODO remove \n",
    "cwd = Path(cwd) \n",
    "datamodel = cwd / 'DataModel'\n",
    "json_files = list(datamodel.glob('*/*.json'))\n",
    "json_data = [json.load(i.open()) for i in json_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Set the current working directory\n",
    "# cwd = r'C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data'  # TODO: remove\n",
    "# cwd = Path(cwd)\n",
    "\n",
    "# # Define the path to the DataModel directory\n",
    "# datamodel = cwd / 'DataModel'\n",
    "\n",
    "# # Find all JSON files in the subdirectories of DataModel\n",
    "# json_files = list(datamodel.glob('*/*.json'))\n",
    "\n",
    "# # Load JSON data from each file\n",
    "# json_data = [json.load(i.open()) for i in json_files]\n",
    "\n",
    "# # Define the path for the output JSON file\n",
    "# output_file = cwd / 'combined_data.json'\n",
    "\n",
    "# # Save the combined JSON data to the output file\n",
    "# with output_file.open('w') as f:\n",
    "#     json.dump(json_data, f, indent=4)\n",
    "\n",
    "# print(f\"Combined JSON data has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data\\reference-table.csv\")\n",
    "df.loc[df['Column Name '] == 'Order Status', 'Column Name '] = 'order_status'\n",
    "df.loc[df['Column Name '] == 'Shipping Method', 'Column Name '] = 'shipping_method'\n",
    "df.to_csv('updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data\\updated.csv\")\n",
    "\n",
    "json_data_ = data.to_json(orient=\"records\")  \n",
    "\n",
    "with open(\"updated_reference.json\", \"w\") as f:\n",
    "    f.write(json_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "NEW_MODEL_NAME = \"NewPHI\"\n",
    "DATASET_NAME = json_data \n",
    "SPLIT = \"train\"\n",
    "MAX_SEQ_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=SPLIT)\n",
    "EOS_TOKEN = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and train model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_model = json.load(open(r\"C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data\\combined_data.json\"))\n",
    "relationships = json.load(open(r\"C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data\\updated_reference.json\"))\n",
    "questions = open(r\"C:\\Users\\raush\\Desktop\\DeskTop\\3.2 years\\Junior AI\\data\\Questions.csv\").readlines()  \n",
    "\n",
    "def extract_keywords(question):\n",
    "    \n",
    "    keywords = question.lower().split()  \n",
    "    return keywords\n",
    "\n",
    "def create_prompt(question):\n",
    "    keywords = extract_keywords(question)\n",
    "\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for table in data_model[\"table_information\"]:\n",
    "        if any(keyword in table[\"table_name\"].lower() or any(keyword in col[\"name\"].lower() for col in table[\"columns\"]) for keyword in keywords):\n",
    "            tables.append(table)\n",
    "            columns.extend(col for col in table[\"columns\"] if any(keyword in col[\"name\"].lower() for keyword in keywords))\n",
    "\n",
    "    prompt = f\"Example Question\\n{question.strip()}\\nExample Output:\\nColumns\\n\"\n",
    "\n",
    "    for table in tables:\n",
    "        prompt += f\"\\n{{\\n\\\"table_name\\\": \\\"{table['table_name']}\\\",\\n\\\"table_description\\\": \\\"{table['table_description']}\\\",\\n\\\"primary_key_column\\\": {table['primary_key_column']},\\n\\\"columns\\\": [\\n\"\n",
    "        for col in table[\"columns\"]:\n",
    "            prompt += f\"{{\\n\\\"name\\\": \\\"{col['name']}\\\",\\n\\\"description\\\": \\\"{col['description']}\\\",\\n\\\"data_type\\\": \\\"{col['data_type']}\\\",\\n\\\"format\\\": \\\"{col['format']}\\\",\\n\\\"is_pii_column\\\": \\\"{col['is_pii_column']}\\\",\\n\\\"enum\\\": {col['enum']},\\n\\\"dimension_group\\\": \\\"{col['dimension_group']}\\\",\\n\\\"is_nullable\\\": \\\"{col['is_nullable']}\\\"\\n}},\\n\"\n",
    "        prompt = prompt.rstrip(\",\\n\")  \n",
    "        prompt += \"\\n]\\n},\\n\"\n",
    "\n",
    "  \n",
    "    prompt += \"\\nRelationship\\n\"\n",
    "    prompt += f\"\\\"relationships\\\": {relationships}\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response\n",
    "model = AutoModelForCausalLM.from_pretrained(model)\n",
    "\n",
    "pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "for question in questions:\n",
    "    prompt = create_prompt(question.strip())\n",
    "    outputs = pipeline(prompt, max_length=50, num_return_sequences=1)\n",
    "    print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
